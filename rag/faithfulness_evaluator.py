# rag/faithfulness_evaluator.py
import re
import logging
from typing import Dict, List, Any, Tuple
import json

logger = logging.getLogger(__name__)


class FaithfulnessEvaluator:
    """
    Evaluates faithfulness of generated answers against retrieved sources.
    
    Faithfulness measures whether the LLM's answer is grounded in the retrieved
    context or contains hallucinations/unsupported claims.
    """
    
    def __init__(self, llm):
        """
        Initialize evaluator with an LLM instance.
        
        Args:
            llm: LLM instance (same as used for answer generation)
        """
        self.llm = llm
        
    def evaluate(
        self, 
        json_response: Dict[str, Any], 
        tagged_context: str,
        source_map: Dict[str, Dict]
    ) -> Dict[str, Any]:
        """
        Evaluate faithfulness of answer against sources.
        
        Args:
            json_response: The JSON answer generated by LLM
            tagged_context: Context with [SOURCE_X] tags
            source_map: Mapping of SOURCE_X to source metadata
            
        Returns:
            Dict with faithfulness score and details
        """
        try:
            # Step 1: Extract statements from answer
            statements = self._extract_statements(json_response)
            
            if not statements:
                logger.warning("No statements extracted from answer")
                return self._create_error_result("No statements found")
            
            # Step 2: Extract context per source
            source_contexts = self._extract_source_contexts(tagged_context)
            
            # Step 3: Evaluate each statement
            total_score = 0.0
            evaluated_count = 0
            statement_details = []
            
            for statement_text, cited_sources in statements:
                if not cited_sources:
                    # Skip statements with no citations
                    continue
                
                # Evaluate against each cited source
                statement_scores = []
                
                for source_id in cited_sources:
                    if source_id not in source_contexts:
                        # Source cited but no context available
                        statement_scores.append(0.0)
                        continue
                    
                    # Get context for this source
                    context = source_contexts[source_id]
                    
                    # Evaluate this statement against this source
                    score = self._evaluate_statement(statement_text, context, source_id)
                    statement_scores.append(score)
                
                # Average across all sources (multi-source handling)
                if statement_scores:
                    avg_score = sum(statement_scores) / len(statement_scores)
                    total_score += avg_score
                    evaluated_count += 1
                    
                    statement_details.append({
                        "statement": statement_text[:100] + "..." if len(statement_text) > 100 else statement_text,
                        "cited_sources": cited_sources,
                        "score": round(avg_score, 2)
                    })
            
            # Step 4: Compute overall score
            if evaluated_count == 0:
                logger.warning("No statements with citations were evaluated")
                return self._create_error_result("No cited statements")
            
            overall_score = total_score / evaluated_count
            
            # Step 5: Determine label (updated thresholds: 80/60 instead of 90/70)
            if overall_score >= 0.80:  # Changed from 0.90
                label = "High"
            elif overall_score >= 0.60:  # Changed from 0.70
                label = "Medium"
            else:
                label = "Low"
            
            # Log summary
            logger.info(f"=== Faithfulness Summary ===")
            logger.info(f"Total statements: {len(statements)}")
            logger.info(f"Evaluated (with citations): {evaluated_count}")
            logger.info(f"Overall score: {overall_score:.2f} ({int(overall_score * 100)}%)")
            logger.info(f"Label: {label}")
            logger.info(f"===========================")
            
            return {
                "score": round(overall_score, 2),
                "label": label,
                "total_statements": len(statements),
                "evaluated_statements": evaluated_count,
                "details": statement_details[:5]  # Only include first 5 for brevity
            }
            
        except Exception as e:
            logger.error(f"Error evaluating faithfulness: {e}", exc_info=True)
            return self._create_error_result(str(e))
    
    def _extract_statements(self, json_response: Dict[str, Any]) -> List[Tuple[str, List[str]]]:
        """
        Extract statements and their citations from JSON answer.
        
        Returns:
            List of (statement_text, [cited_sources]) tuples
        """
        statements = []
        
        # Extract from overview
        overview = json_response.get("overview", "")
        if overview:
            statements.append((overview, self._extract_citations(overview)))
        
        # Extract from sections
        sections = json_response.get("sections", [])
        for section in sections:
            # Section content (paragraph format)
            if "content" in section:
                content = section["content"]
                if content:
                    statements.append((content, self._extract_citations(content)))
            
            # Section items (bullet points)
            if "items" in section:
                for item in section["items"]:
                    if isinstance(item, dict):
                        # Extract all text fields from item
                        for key, value in item.items():
                            if isinstance(value, str) and value and key != "frequency":
                                statements.append((value, self._extract_citations(value)))
        
        return statements
    
    def _extract_citations(self, text: str) -> List[str]:
        """
        Extract [SOURCE_X] citations from text.
        
        Args:
            text: Text containing [SOURCE_X] tags
            
        Returns:
            List of source IDs (e.g., ["SOURCE_1", "SOURCE_5"])
        """
        pattern = r'\[SOURCE_(\d+)\]'
        matches = re.findall(pattern, text)
        return [f"SOURCE_{num}" for num in matches]
    
    def _extract_source_contexts(self, tagged_context: str) -> Dict[str, str]:
        """
        Extract context for each source from tagged context string.
        
        Args:
            tagged_context: String with [SOURCE_X] tags
            
        Returns:
            Dict mapping SOURCE_X to its context text
        """
        source_contexts = {}
        
        # Split by [SOURCE_X] tags
        pattern = r'\[SOURCE_(\d+)\]'
        parts = re.split(pattern, tagged_context)
        
        # parts will be: ['', '1', 'text for source 1', '2', 'text for source 2', ...]
        for i in range(1, len(parts), 2):
            if i + 1 < len(parts):
                source_id = f"SOURCE_{parts[i]}"
                context_text = parts[i + 1].strip()
                
                # Append to existing context if source appears multiple times
                if source_id in source_contexts:
                    source_contexts[source_id] += "\n\n" + context_text
                else:
                    source_contexts[source_id] = context_text
        
        return source_contexts
    
    def _evaluate_statement(self, statement: str, context: str, source_id: str) -> float:
        """
        Evaluate if a statement can be inferred from the given context.
        
        Args:
            statement: The claim to verify
            context: The source context to check against
            source_id: ID of the source (for logging)
            
        Returns:
            Score: 1.0 (faithful), 0.5 (partial), 0.0 (not faithful)
        """
        # Remove any [SOURCE_X] tags from statement for cleaner evaluation
        clean_statement = re.sub(r'\[SOURCE_\d+\]', '', statement).strip()
        
        # Truncate statement if very long (keep first 150 chars)
        if len(clean_statement) > 150:
            clean_statement = clean_statement[:147] + "..."
        
        evaluation_prompt = f"""You are evaluating whether a medical answer statement is faithful to source material.

SOURCE CONTEXT:
{context[:2000]}

STATEMENT TO VERIFY:
{clean_statement}

TASK:
Determine if the statement is supported by the source context.

IMPORTANT GUIDELINES:
- Medical summaries often paraphrase technical details - this is acceptable
- Slight rewording or simplification does NOT mean unfaithful
- Focus on FACTUAL ACCURACY, not exact word matching
- If the general medical idea is present in the context → FAITHFUL
- Only mark NOT_FAITHFUL if the statement contradicts or adds unsupported claims
- PARTIAL means the statement is mostly supported but adds minor unsupported details

RULES:
- FAITHFUL: The statement's facts are supported by or can be reasonably inferred from the context
- PARTIAL: The statement is mostly supported but includes some details not clearly in the context
- NOT_FAITHFUL: The statement contradicts the context or makes claims clearly not present

Respond with ONLY ONE WORD: FAITHFUL, PARTIAL, or NOT_FAITHFUL

Your answer:"""
        
        try:
            response = self.llm.ask(evaluation_prompt).strip().upper()
            
            # Parse response and assign score
            if "FAITHFUL" in response and "NOT" not in response and "PARTIAL" not in response:
                score = 1.0
                result_label = "✅ FAITHFUL"
            elif "PARTIAL" in response:
                score = 0.5
                result_label = "⚠️ PARTIAL"
            else:
                score = 0.0
                result_label = "❌ NOT_FAITHFUL"
            
            # Debug logging - show first 60 chars of statement
            statement_preview = clean_statement[:60] + "..." if len(clean_statement) > 60 else clean_statement
            logger.info(f"{result_label} ({score:.1f}): '{statement_preview}' [{source_id}]")
            
            return score
                
        except Exception as e:
            logger.error(f"Error evaluating statement against {source_id}: {e}")
            # On error, assume neutral (partial) rather than failing
            return 0.5
    
    def _create_error_result(self, error_msg: str) -> Dict[str, Any]:
        """
        Create a result dict for error cases.
        """
        return {
            "score": None,
            "label": "Not Available",
            "total_statements": 0,
            "evaluated_statements": 0,
            "error": error_msg,
            "details": []
        }
